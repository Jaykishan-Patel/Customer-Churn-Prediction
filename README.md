ğŸ“Š Customer Churn Prediction using Machine Learning
ğŸ“Œ Project Overview

Customer churn is a major challenge in the telecom industry, as acquiring new customers is significantly more expensive than retaining existing ones.
This project aims to predict whether a customer will churn or not using historical customer data, enabling businesses to take proactive retention actions.

The solution is built as an end-to-end, leakage-free machine learning pipeline, focusing on Recall and F1-score to align with real business objectives.

ğŸ¯ Objectives

Predict customer churn (Yes / No)

Identify high-risk customers in advance

Reduce revenue loss through data-driven decision making

Build a production-ready ML pipeline

ğŸ§  Problem Statement

Given customer demographic information, service usage details, and billing data, the task is to build a classification model that accurately predicts customer churn.
Since churn data is imbalanced, accuracy alone is not sufficient; therefore, Recall and F1-score are prioritized.

ğŸ“‚ Dataset

Source: Kaggle (Telco Customer Churn Dataset)

Size: 7,043 rows Ã— 21 columns

Target Variable: Churn

1 â†’ Customer churned

0 â†’ Customer retained

ğŸ› ï¸ Tech Stack

Programming: Python

Libraries: Pandas, NumPy, Scikit-learn, XGBoost, Imbalanced-learn

Visualization: Matplotlib, Seaborn

Model Saving: joblib

ğŸ” Key Steps in the Project
1ï¸âƒ£ Data Preprocessing

Removed duplicate and irrelevant columns

Handled missing values and corrected data types

Converted TotalCharges to numeric

Binned tenure into categorical groups

Applied One-Hot Encoding for categorical variables

Scaled numerical features where required

2ï¸âƒ£ Exploratory Data Analysis (EDA)

Analyzed numerical and categorical features

Identified churn patterns based on tenure, contract type, and monthly charges

Observed strong class imbalance (â‰ˆ 73:27)

3ï¸âƒ£ Handling Imbalanced Data

Applied SMOTE / SMOTEENN to improve minority class (churn) prediction

Compared model performance before and after resampling

4ï¸âƒ£ Model Building

Trained and evaluated multiple models:

Logistic Regression

Support Vector Machine (SVM)

Decision Tree

Random Forest

Gradient Boosting

XGBoost

Used Pipeline and ColumnTransformer to prevent data leakage.

5ï¸âƒ£ Model Evaluation

Cross-validation for stable performance

Metrics used:

Precision

Recall

F1-score

Confusion Matrix

Recall and F1-score prioritized due to business importance

6ï¸âƒ£ Final Model Selection

Random Forest selected as the final model

Provided the best balance of Recall and F1-score

Evaluated on test data using confusion matrix and classification report

ğŸ“ˆ Model Performance (Final Model)
Metric	Value
Accuracy	0.72
Precision (Churn)	0.48
Recall (Churn)	0.82
F1-score (Churn)	0.61

âœ” High recall ensures most churn-prone customers are identified.

ğŸ’¾ Model Deployment

Final pipeline saved using joblib

Supports prediction on new/unseen customer data

Ensures consistent preprocessing during inference

joblib.dump(final_rf_pipeline, "churn_random_forest_pipeline.pkl")

ğŸ”® Predicting for New Customers

Load the saved pipeline

Provide customer details in the same format as training data

Pipeline automatically handles preprocessing and prediction

ğŸ“Œ What I Learned

How to solve a real-world imbalanced classification problem

Importance of Recall and F1-score over accuracy

Preventing data leakage using pipelines

Handling class imbalance with SMOTE/SMOTEENN

Model comparison and business-driven model selection

Saving and deploying ML models for real-world use

ğŸš€ Future Improvements

Hyperparameter tuning using GridSearchCV

Threshold optimization for better precisionâ€“recall tradeoff

Deploy model using Flask / FastAPI

Monitor model performance and data drift

ğŸ‘¤ Author

Jaykishan Patel
Aspiring Data Scientist / ML Engineer
